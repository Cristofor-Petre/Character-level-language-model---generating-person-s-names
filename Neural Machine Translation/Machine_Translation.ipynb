{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa36X4Ye1LCq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a2d35ec6-f2b0-44cc-9258-d548132bf038"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hx_PyCkZtZ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "b7b67429-681e-4796-fd79-ceca4c97811b"
      },
      "source": [
        "!pip3 install unidecode"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 15.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_MM-fnobJRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"your-path/Machine Translation\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "algF29r1xqgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71e356b3-7534-48e4-d6b8-88b879581726"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Input, Dropout, GRU, Embedding, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model\n",
        "from unidecode import unidecode\n",
        "import re\n",
        "import string\n",
        "from pickle import dump, load\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from collections import Counter\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import model_from_json\n",
        "from os import path\n",
        "from DataGenerator import DataGenerator"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4hRzIdL1iEk",
        "colab_type": "text"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJHPqWVG8lPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NMT:\n",
        "\n",
        "\tdef __init__(self, dataset_english_path, dataset_french_path, clean_en_path, clean_fr_path, encoder_input_path, \n",
        "\t             decoder_input_path, decoder_output_path, vocab_path, max_sentence_length):\n",
        "\t\t\t\n",
        "\t\tself.dataset_english_path = dataset_english_path\n",
        "\t\tself.dataset_french_path = dataset_french_path\n",
        "\t\tself.clean_en_path = clean_en_path\n",
        "\t\tself.clean_fr_path = clean_fr_path\n",
        "\t\tself.encoder_input_path = encoder_input_path\n",
        "\t\tself.decoder_input_path = decoder_input_path\n",
        "\t\tself.decoder_output_path = decoder_output_path\n",
        "\t\tself.max_sentence_length = max_sentence_length\n",
        "\t\tself.vocab_path = vocab_path\n",
        "\n",
        "\t\tif path.exists(self.clean_en_path) and  path.exists(self.clean_fr_path):\n",
        "\t\t\t\tself.dataset_english = self.load_prep_sentences(self.clean_en_path)\n",
        "\t\t\t\tself.dataset_french = self.load_prep_sentences(self.clean_fr_path)\n",
        "\t\telse:\n",
        "\t\t\t\tself.dataset_english = self.load_doc(self.dataset_english_path)\n",
        "\t\t\t\tself.dataset_french = self.load_doc(self.dataset_french_path)\n",
        "\t\t\t\tself.save_prep_sentences(self.dataset_english, self.clean_en_path)\n",
        "\t\t\t\tself.save_prep_sentences(self.dataset_french, self.clean_fr_path)\n",
        "\n",
        "\n",
        "\n",
        "\t\tif path.exists(self.encoder_input_path) and path.exists(self.decoder_input_path) and path.exists(self.decoder_output_path):\n",
        "\t\t\t\n",
        "\t\t\tprint(\"Loading preprocessed datasets...\\n\")\n",
        "\t \n",
        "\t\t\tself.encoder_input = self.load_prep_sentences(self.encoder_input_path)\n",
        "\t\t\tself.decoder_input = self.load_prep_sentences(self.decoder_input_path)\n",
        "\t\t\tself.decoder_output = self.load_prep_sentences(self.decoder_output_path)\n",
        "\t\t\tself.vocab_dict = self.load_prep_sentences(self.vocab_path)\n",
        "\n",
        "\t\t\tprint(\"Loaded succesfully!\\n\")\n",
        "\t\telse:\n",
        "\n",
        "\t\t\tself.encoder_input, self.decoder_input, self.decoder_output = self.prepare_dataset(self.dataset_english, self.dataset_french,\n",
        "\t\t\t                                                                                   keep_all = True)\n",
        "\t\t\tself.save_prep_sentences(self.encoder_input,self.encoder_input_path)\n",
        "\t\t\tself.save_prep_sentences(self.decoder_input,self.decoder_input_path)\n",
        "\t\t\tself.save_prep_sentences(self.decoder_output,self.decoder_output_path)\n",
        "\t\t\tself.save_prep_sentences(self.vocab_dict, self.vocab_path)\n",
        "\n",
        "\n",
        "\t\n",
        "\tdef prepare_dataset(self, dataset_english, dataset_french, keep_all = True):\n",
        "\n",
        "\t\t\n",
        "\t\tsentences_fr = self.dataset_french.copy()\n",
        "\t\tsentences_eng = self.dataset_english.copy()\n",
        "\n",
        "\n",
        "\t\tif not keep_all:\n",
        "\t\t\tsentences_eng = sentences_eng[:1000]\n",
        "\t\t\tsentences_fr = sentences_fr[:1000]\n",
        "\t\t\n",
        "\n",
        "\t\tsentences_eng, sentences_fr = self.reduce_sequences(sentences_eng, sentences_fr)\n",
        "\t\n",
        "\t\tsentences_fr, decoder_input_fr, decoder_output_fr = self.insert_start_stop(sentences_fr)\n",
        "\t\tfr_vocab_size, vocab_fr, decoder_input_fr, decoder_output_fr = self.tokenize(sentences_fr, decoder_input_fr, decoder_output_fr)\n",
        "\t\n",
        "\t\teng_vocab_size, vocab_eng, encoder_eng = self.tokenize(sentences_eng)\n",
        "\n",
        "\t\tself.vocab_dict = {\n",
        "\t\t\t\t\"vocab_fr\" : vocab_fr,\n",
        "\t\t\t\t\"fr_vocab_size\" : fr_vocab_size,\n",
        "\t\t\t\t\"vocab_eng\" : vocab_eng,\n",
        "\t\t\t\t\"eng_vocab_size\" : eng_vocab_size\n",
        "\t\t}\n",
        "\n",
        "\n",
        "\t\tencoder_padded = self.pad_seq(encoder_eng)\n",
        "\t\tdecoder_input_padded = self.pad_seq(decoder_input_fr)\n",
        "\t\tdecoder_output_padded = self.pad_seq(decoder_output_fr)\n",
        "\t\n",
        "\n",
        "\t\treturn encoder_padded, decoder_input_padded, decoder_output_padded\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tdef load_doc(self,filename):\n",
        "\t\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t\ttext = file.read()\n",
        "\t\tfile.close()\n",
        "\t\n",
        "\t\treturn self.clean_sentences(text)\n",
        "\n",
        "\tdef clean_sentences(self,doc):\n",
        "\t\tsentences =  doc.strip().split('\\n')\n",
        "\t\tsentences = [re.sub(r\"[^\\w\\s]\",\"\",unidecode(sent.lower())) for sent in sentences]\n",
        "\t\treturn sentences\n",
        "\n",
        "\n",
        "\tdef save_prep_sentences(self,sentences, filename):\n",
        "\t\tdump(sentences, open(filename, 'wb'))\n",
        "\t\tprint('Saved: ', filename)\n",
        "\n",
        "\n",
        "\tdef load_prep_sentences(self,filename):\n",
        "\t\treturn load(open(filename, 'rb'))\n",
        "\t\n",
        "\n",
        "\tdef insert_start_stop(self,target):\n",
        "\n",
        "\t\t\t'''Insert \"start\" string at the beginning of each sequence from decoder_input and \"stop\"\n",
        "\t\t at the end of each sequence from decoder_output. We do this in order to enforce teacher forcing (where\n",
        "\t\t the decoder is trained to predict the next word given the previous words of the sentence).\n",
        "\t\t\t'''\n",
        "\t\tdecoder_input = []\n",
        "\t\tdecoder_output = []\n",
        "\n",
        "\t\tfor i,st in enumerate(target):\n",
        "\t\t\ttarget[i] = \" \".join((\"start\",st, \"stop\"))\n",
        "\t\t\tdecoder_input.append(\" \".join((\"start\",st)))\n",
        "\t\t\tdecoder_output.append(\" \".join((st, \"stop\")))\n",
        "\t\t\n",
        "\t\treturn target, decoder_input, decoder_output\n",
        "\n",
        "\tdef reduce_vocabulary(self,sequence, min_occurence):\n",
        "\t\t'''\n",
        "\t\tKeep only the words that occur at least min_occurence times in \n",
        "\t\teng/fr sentences in order to reduce the vocabulary size (if needed)\n",
        "\t\t'''\n",
        "\t\tcount_obj = Counter()\n",
        "\t\n",
        "\t\tfor seq in sequence:\n",
        "\t\t\tcount_obj.update(seq.split())\n",
        "\t\t\n",
        "\t\tnr_words_to_keep = 0\n",
        "\t\tfor word, nr_occurences in count_obj.items():\n",
        "\t\t\tif nr_occurences >= min_occurence:\n",
        "\t\t\t\tnr_words_to_keep += 1\n",
        "\n",
        "\t\t\n",
        "\t\treturn nr_words_to_keep\n",
        "\n",
        "\t\n",
        "\n",
        "\tdef tokenize(self,list_sentences, *args):\n",
        "\t\t\n",
        "\t\t'''Create vocabulary and transform text sentences into number sequences based on the vocabulary'''\n",
        "\n",
        "\t\tmin_occurence = 1\n",
        "\t\tnum_words = self.reduce_vocabulary(list_sentences, min_occurence)\n",
        "\n",
        "\n",
        "\t\ttok = Tokenizer(num_words = num_words + 1, oov_token='OOV')\n",
        "\t\ttok.fit_on_texts(list_sentences)\n",
        "\t\tvocab = tok.word_index\n",
        "\n",
        "\t\tif len(args) == 0:\n",
        "\t\t\treturn len(vocab), vocab, tok.texts_to_sequences(list_sentences)\n",
        "\n",
        "\t\treturn len(vocab), vocab, tok.texts_to_sequences(args[0]), tok.texts_to_sequences(args[1])\n",
        "\n",
        "\n",
        "\n",
        "\tdef max_len(self,sequences):\n",
        "\t\treturn max([len(s.split()) for s in sequences])\n",
        "\n",
        "\n",
        "\n",
        "\tdef reduce_sequences(self, sentences_eng, sentences_fr):\n",
        "\t\t'''Reduce the number of sequences by imposing a max-length on each eng & fr pair of sentences'''\n",
        "\n",
        "\t\t\tsent_eng = []\n",
        "\t\t\tsent_fr = []\n",
        "\n",
        "\t\t\tfor s_en, s_fr in zip(sentences_eng, sentences_fr):\n",
        "\t\t\t\tif len(s_en.split()) <= self.max_sentence_length and len(s_fr.split()) <= self.max_sentence_length and \\\n",
        "\t\t\t\t\tlen(s_en.split()) > 0 and len(s_fr.split()) > 0:\n",
        "\t\t\t\t\tsent_eng.append(s_en)\n",
        "\t\t\t\t\tsent_fr.append(s_fr)\n",
        "\t\t\t\n",
        "\t\t\tassert self.max_len(sent_eng) <= self.max_sentence_length\n",
        "\t\t\tassert self.max_len(sent_fr) <= self.max_sentence_length\n",
        "\n",
        "\t\t\treturn sent_eng, sent_fr\n",
        "\n",
        "\n",
        "\tdef pad_seq(self,sequences):\n",
        "\t\treturn pad_sequences(sequences, padding = 'post')\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5n5sPNMTxoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_english_path = r\"your-path/small_vocab_en.txt\"\n",
        "dataset_french_path = r\"your-path/small_vocab_fr.txt\"\n",
        "encoder_input_path = r\"your-path/encoder_input.plk\"\n",
        "decoder_input_path = r\"your-path/decoder_input.plk\"\n",
        "encoder_output_path = r\"your-path/decoder_output.plk\"\n",
        "clean_en_path = r\"your-path/clean_en_git.plk\"\n",
        "clean_fr_path = r\"your-path/clean_fr_git.plk\"\n",
        "vocab_path = r\"your-path/vocab.plk\"\n",
        "max_sentence_length = 30\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R399wLnGUeSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmt = NMT(dataset_english_path, dataset_french_path, clean_en_path, clean_fr_path, encoder_input_path, decoder_input_path, \n",
        "          encoder_output_path, vocab_path, max_sentence_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwaBLqlooGWk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "8c145d0f-0f00-4a4f-fff5-baa3efeab61d"
      },
      "source": [
        "print(\"Number of english sentences: {}\\n\".format(len(nmt.encoder_input)))\n",
        "print(\"Number of french sentences: {}\\n\".format(len(nmt.decoder_input)))\n",
        "print(\"The length of english vocabulary: {}\\n\".format(nmt.vocab_dict['eng_vocab_size']))\n",
        "print(\"The length of french vocabulary: {}\\n\".format(nmt.vocab_dict['fr_vocab_size']))\n",
        "  "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of english sentences: 137860\n",
            "\n",
            "Number of french sentences: 137860\n",
            "\n",
            "The length of english vocabulary: 200\n",
            "\n",
            "The length of french vocabulary: 342\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oucM9rddGW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_english_dict = dict((i, word) for word, i in nmt.vocab_dict[\"vocab_eng\"].items())\n",
        "reverse_french_dict = dict((i, word) for word, i in nmt.vocab_dict[\"vocab_fr\"].items())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJD2GyUIrDXa",
        "colab_type": "text"
      },
      "source": [
        "# Encoder-Decoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxCoObBJiJoi",
        "colab_type": "text"
      },
      "source": [
        "## Split the data into training, validation, testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ0A1bGk8LOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_train = nmt.encoder_input[:int(len(nmt.encoder_input) * 0.90)]\n",
        "encoder_validation = nmt.encoder_input[int(len(nmt.encoder_input) * 0.90):]\n",
        "\n",
        "decoder_input_train = nmt.decoder_input[:int(len(nmt.decoder_input) * 0.90)]\n",
        "decoder_input_validation = nmt.decoder_input[int(len(nmt.decoder_input) * 0.90):]\n",
        "\n",
        "decoder_output_train = nmt.decoder_output[:int(len(nmt.decoder_output) * 0.90)]\n",
        "decoder_output_validation = nmt.decoder_output[int(len(nmt.decoder_output) * 0.90) : ]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMMqMgeDsea0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_units = 256\n",
        "\n",
        "num_encoder_tokens = encoder_train.shape[1]\n",
        "num_decoder_tokens = decoder_input_train.shape[1]\n",
        "num_classes = nmt.vocab_dict['fr_vocab_size']\n",
        "\n",
        "training_generator = DataGenerator(encoder_train, decoder_input_train, decoder_output_train, 1024, num_classes)\n",
        "validation_generator = DataGenerator(encoder_validation, decoder_input_validation, decoder_output_validation, 1024, num_classes)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1c-mXNA34vb",
        "colab_type": "text"
      },
      "source": [
        "# Model with embedding for decoder input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s3kENgxb5WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''encoder_inputs = Input(shape=(None,), name = \"encoder_input\")\n",
        "\n",
        "encoder_embeddings = Embedding(input_dim = nmt.vocab_dict['eng_vocab_size'], output_dim = 200, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(n_units, return_sequences = False, return_state=True, activation='relu')\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,), name = \"decoder_input\")\n",
        "decoder_embedding = Embedding(input_dim = nmt.vocab_dict['fr_vocab_size'], output_dim = 200, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = LSTM(n_units, return_state = True, return_sequences=True, activation='relu')\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(num_classes, activation='softmax', name = \"decoder_dense\")\n",
        "decoder_logits = decoder_dense(decoder_outputs)\n",
        "\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_logits)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.003), loss='categorical_crossentropy', metrics=['accuracy'])'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JxxC_iP4FRH",
        "colab_type": "text"
      },
      "source": [
        "# Model without embedding the decoder input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzGBTvv23c5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_inputs = Input(shape=(None,), name = \"encoder_input\")\n",
        "\n",
        "encoder_embeddings = Embedding(input_dim = nmt.vocab_dict['eng_vocab_size'], output_dim = 200, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(n_units, return_sequences = False, return_state=True, activation='relu')\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embeddings)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,num_classes), name = \"decoder_input\")\n",
        "decoder_lstm = LSTM(n_units, return_state = True, return_sequences=True, activation='relu')\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_classes, activation='softmax', name = \"decoder_dense\")\n",
        "decoder_logits = decoder_dense(decoder_outputs)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_logits)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.003), loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_QAgH_y4XV4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "3843f096-8e78-4bc7-e216-7b995c3de2e0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 200)    40000       encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input (InputLayer)      (None, None, 342)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 256), (None, 467968      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 256),  613376      decoder_input[0][0]              \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_dense (Dense)           (None, None, 342)    87894       lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,209,238\n",
            "Trainable params: 1,209,238\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHvOaL2y09X_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "10420dcc-b3ad-4e36-f1cd-48f9e81c88b7"
      },
      "source": [
        "history = model.fit_generator(generator = training_generator, validation_data = validation_generator,epochs = 5)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "121/121 [==============================] - 217s 2s/step - loss: 0.0263 - accuracy: 0.9924 - val_loss: 0.0289 - val_accuracy: 0.9927\n",
            "Epoch 2/5\n",
            "121/121 [==============================] - 218s 2s/step - loss: 0.0219 - accuracy: 0.9936 - val_loss: 0.0234 - val_accuracy: 0.9933\n",
            "Epoch 3/5\n",
            "121/121 [==============================] - 218s 2s/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 0.0181 - val_accuracy: 0.9938\n",
            "Epoch 4/5\n",
            "121/121 [==============================] - 217s 2s/step - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.0210 - val_accuracy: 0.9944\n",
            "Epoch 5/5\n",
            "121/121 [==============================] - 216s 2s/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.0179 - val_accuracy: 0.9944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-0ygJKaMIIP",
        "colab_type": "text"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLo-IasT4OJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"your-path/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "model.save_weights(\"your-path/model.h5\")\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0lXa6MUMLQw",
        "colab_type": "text"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxGull5YgdS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_file = open('your-path/model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "loaded_model.load_weights(\"your-path/Machine Translation/model.h5\")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSUoXNwaUI0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "5fc18306-865f-48b1-89dc-d9ba714ac192"
      },
      "source": [
        "loaded_model.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 200)    40000       encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "decoder_input (InputLayer)      (None, None, 342)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 256), (None, 467968      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 256),  613376      decoder_input[0][0]              \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_dense (Dense)           (None, None, 342)    87894       lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,209,238\n",
            "Trainable params: 1,209,238\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhzZRR03kJAO",
        "colab_type": "text"
      },
      "source": [
        "# Inference step\n",
        "<ul>\n",
        "<li>Retrieve the output of the layers from the loaded model to be used for inference models</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUqNTXr_uryu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_inputs = loaded_model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = loaded_model.layers[3].output #lstm1\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma-AB4ycUF05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "decoder_inputs = loaded_model.input[1]\n",
        "decoder_lstm = loaded_model.layers[4] #lstm2\n",
        "decoder_dense = loaded_model.layers[5]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCU-xzCykL6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(n_units,))\n",
        "decoder_state_input_c = Input(shape=(n_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmFqH635Qa6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_logits = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_logits] + decoder_states)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaCr_Gd4rOvZ",
        "colab_type": "text"
      },
      "source": [
        "## Decode sequences / Make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oxdgTEwrRyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1,1,num_classes))\n",
        "\n",
        "    target_seq[0,0,nmt.vocab_dict[\"vocab_fr\"]['start']] = 1\n",
        "\n",
        "\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0,0, :])\n",
        "        sampled_word = reverse_french_dict[sampled_token_index]\n",
        "        decoded_sentence += sampled_word + ' '\n",
        "\n",
        "\n",
        "        if (sampled_word == 'stop' or len(decoded_sentence) > 30):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1,1,num_classes))\n",
        "        target_seq[0,0,sampled_token_index] = 1\n",
        "\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnCdmXarb4pP",
        "colab_type": "text"
      },
      "source": [
        "<ul>\n",
        "<li>Function to convert number sequences back into their corresponding strings</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrZ9WqdDeEJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_to_word(seq, rev):\n",
        "  dec = ' '\n",
        "  for idx in seq[seq != 0]:\n",
        "    dec += rev[idx] + ' '\n",
        "  \n",
        "  return dec\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXUKdaJebxU1",
        "colab_type": "text"
      },
      "source": [
        "<ul>\n",
        "<li>Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jijxn5Lb03VC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def levenshtein(a,b):\n",
        "    m = np.zeros((len(a)+1, len(b)+1))\n",
        "\n",
        "    for i in range(1,m.shape[0]):\n",
        "      m[i,0] = m[i-1,0] + 1\n",
        "\n",
        "    for i in range(1,m.shape[1]):\n",
        "      m[0,i] = m[0,i-1] + 1\n",
        "    \n",
        "    for i in range(1,m.shape[0]):\n",
        "      for j in range(1, m.shape[1]):\n",
        "        if a[i-1] != b[j-1]:\n",
        "          m[i,j] = min(m[i-1,j], m[i,j-1], m[i-1,j-1]) + 1\n",
        "        else:\n",
        "          m[i,j] = m[i-1,j-1]\n",
        "    \n",
        "    return m[-1,-1]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGCtDuE8_Nqb",
        "colab_type": "text"
      },
      "source": [
        "## Validation set predictions\n",
        "As we can see below, since only a limited number of sentences (on validation data) have a levenshtein distance $\\leq 4$, we assume that our model is under-fitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9r4gvAGWS-u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0eaf3da0-1069-4366-9a47-a8996c25fdc3"
      },
      "source": [
        "\n",
        "for i in range(len(encoder_validation)):\n",
        "  inp = index_to_word(encoder_validation[i], reverse_english_dict)\n",
        "  gt = index_to_word(decoder_input_validation[i], reverse_french_dict)[6:]\n",
        "  pred = decode_sequence(encoder_validation[i])[:-5]\n",
        "  if levenshtein(gt.split(), pred.split()) / len(gt.split()) <= 0.4:\n",
        "    print('\\nInput: {}\\n'.format(inp))\n",
        "    print('Ground truth: {}\\n'.format(gt))\n",
        "    print(\"Predicted: {}\\n\".format(pred))\n",
        "    print(\"-\" * 20)\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input:  i dislike oranges \n",
            "\n",
            "Ground truth:  je naime les oranges \n",
            "\n",
            "Predicted: je naime les mangues \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  i dislike grapes \n",
            "\n",
            "Ground truth:  je naime pas les raisins \n",
            "\n",
            "Predicted: je naime les mangues \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved animal is this bear \n",
            "\n",
            "Ground truth:  son animal le plus aime est cet ours \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved animal is this bear \n",
            "\n",
            "Ground truth:  mon animal le plus aime est cet ours \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved animal is the horse \n",
            "\n",
            "Ground truth:  mon animal le plus aime est le cheval \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved animal is this lion \n",
            "\n",
            "Ground truth:  son animal le plus aime est ce lion \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  i dislike strawberries \n",
            "\n",
            "Ground truth:  je naime pas les fraises \n",
            "\n",
            "Predicted: je naime les mangues \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  i dislike pears \n",
            "\n",
            "Ground truth:  je naime pas les poires \n",
            "\n",
            "Predicted: je naime les mangues \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved animal is this mouse \n",
            "\n",
            "Ground truth:  mon animal le plus aime est cette souris \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved animal is that bird \n",
            "\n",
            "Ground truth:  mon animal le plus aime est cet oiseau \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved animal is this bird \n",
            "\n",
            "Ground truth:  mon animal le plus aime est cet oiseau \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Pi9k_G_VrU",
        "colab_type": "text"
      },
      "source": [
        "## Training set predictions\n",
        "To check the assumption of under-fitting i have also printed the predictions on the training data, and indeed we confirm it since, again, a limited number of the predictions have a decent levenshtein distance with respect to the original sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3AXxadgXmRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "773ee5fb-994b-4997-d633-7501755a1b72"
      },
      "source": [
        "for i in range(len(encoder_train)):\n",
        "  inp = index_to_word(encoder_train[i], reverse_english_dict)\n",
        "  gt = index_to_word(decoder_input_train[i], reverse_french_dict)[6:]\n",
        "  pred = decode_sequence(encoder_train[i])[:-5]\n",
        "  if levenshtein(gt.split(), pred.split()) / len(gt.split()) <= 0.3:\n",
        "    print('\\nInput: {}\\n'.format(inp))\n",
        "    print('Ground truth: {}\\n'.format(gt))\n",
        "    print(\"Predicted: {}\\n\".format(pred))\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input:  her most loved fruit is the grape \n",
            "\n",
            "Ground truth:  son fruit le plus aime est le raisin \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved fruit is the peach \n",
            "\n",
            "Ground truth:  son fruit le plus aime est la peche \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  they like lemons \n",
            "\n",
            "Ground truth:  ils aiment les citrons \n",
            "\n",
            "Predicted: ils aiment les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the grape \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est le raisin \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved fruit is the mango \n",
            "\n",
            "Ground truth:  son fruit le plus aime est la mangue \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  we like lemons \n",
            "\n",
            "Ground truth:  nous aimons les citrons \n",
            "\n",
            "Predicted: nous aimons les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved fruit is the lime \n",
            "\n",
            "Ground truth:  son fruit le plus aime est la chaux \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved fruit is the strawberry \n",
            "\n",
            "Ground truth:  son fruit le plus aime est la fraise \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved fruit is the grapefruit \n",
            "\n",
            "Ground truth:  son fruit le plus aime est le pamplemousse \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the grapefruit \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est le pamplemousse \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the peach \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est la peche \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  they like mangoes \n",
            "\n",
            "Ground truth:  ils aiment les mangues \n",
            "\n",
            "Predicted: ils aiment les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  they dislike lemons \n",
            "\n",
            "Ground truth:  ils naiment les citrons \n",
            "\n",
            "Predicted: ils aiment les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the lemon \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est le citron \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  we like peaches \n",
            "\n",
            "Ground truth:  nous aimons les peches \n",
            "\n",
            "Predicted: nous aimons les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  you like oranges \n",
            "\n",
            "Ground truth:  vous aimez les oranges \n",
            "\n",
            "Predicted: vous aimez les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved fruit is the lemon \n",
            "\n",
            "Ground truth:  son fruit le plus aime est le citron \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved fruit is the orange \n",
            "\n",
            "Ground truth:  son fruit le plus aime est lorange \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved fruit is the banana \n",
            "\n",
            "Ground truth:  son fruit le plus aime est la banane \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  you like peaches \n",
            "\n",
            "Ground truth:  vous aimez les peches \n",
            "\n",
            "Predicted: vous aimez les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the orange \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est lorange \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the apple \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est la pomme \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the strawberry \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est la fraise \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  you like mangoes \n",
            "\n",
            "Ground truth:  vous aimez les mangues \n",
            "\n",
            "Predicted: vous aimez les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the banana \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est la banane \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  we like apples \n",
            "\n",
            "Ground truth:  nous aimons les pommes \n",
            "\n",
            "Predicted: nous aimons les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved animal is the bird \n",
            "\n",
            "Ground truth:  mon animal le plus aime est loiseau \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  we like grapes \n",
            "\n",
            "Ground truth:  nous aimons les raisins \n",
            "\n",
            "Predicted: nous aimons les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  they like oranges \n",
            "\n",
            "Ground truth:  ils aiment les oranges \n",
            "\n",
            "Predicted: ils aiment les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  you like lemons \n",
            "\n",
            "Ground truth:  vous aimez les citrons \n",
            "\n",
            "Predicted: vous aimez les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  you like apples \n",
            "\n",
            "Ground truth:  vous aimez les pommes \n",
            "\n",
            "Predicted: vous aimez les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  you like strawberries \n",
            "\n",
            "Ground truth:  vous aimez les fraises \n",
            "\n",
            "Predicted: vous aimez les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved fruit is the apple \n",
            "\n",
            "Ground truth:  son fruit le plus aime est la pomme \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved animal is the bear \n",
            "\n",
            "Ground truth:  mon animal le plus aime est lours \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the pear \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est la poire \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  we like oranges \n",
            "\n",
            "Ground truth:  nous aimons les oranges \n",
            "\n",
            "Predicted: nous aimons les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  we like mangoes \n",
            "\n",
            "Ground truth:  nous aimons les mangues \n",
            "\n",
            "Predicted: nous aimons les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  we dislike lemons \n",
            "\n",
            "Ground truth:  nous detestons les citrons \n",
            "\n",
            "Predicted: nous aimons les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved animal is the elephant \n",
            "\n",
            "Ground truth:  mon animal le plus aime est lelephant \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  they like pears \n",
            "\n",
            "Ground truth:  ils aiment les poires \n",
            "\n",
            "Predicted: ils aiment les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  her most loved fruit is the pear \n",
            "\n",
            "Ground truth:  son fruit le plus aime est la poire \n",
            "\n",
            "Predicted: son fruit le plus aime est la chaux \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the lime \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est la chaux \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  they like peaches \n",
            "\n",
            "Ground truth:  ils aiment les peches \n",
            "\n",
            "Predicted: ils aiment les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  we like strawberries \n",
            "\n",
            "Ground truth:  nous aimons les fraises \n",
            "\n",
            "Predicted: nous aimons les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  they like apples \n",
            "\n",
            "Ground truth:  ils aiment les pommes \n",
            "\n",
            "Predicted: ils aiment les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  they like bananas \n",
            "\n",
            "Ground truth:  ils aiment les bananes \n",
            "\n",
            "Predicted: ils aiment les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  california is warm during january \n",
            "\n",
            "Ground truth:  californie est chaud en janvier \n",
            "\n",
            "Predicted: californie est chaud en juillet \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  you like bananas \n",
            "\n",
            "Ground truth:  vous aimez les bananes \n",
            "\n",
            "Predicted: vous aimez les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  they like grapes \n",
            "\n",
            "Ground truth:  ils aiment les raisins \n",
            "\n",
            "Predicted: ils aiment les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  they like strawberries \n",
            "\n",
            "Ground truth:  ils aiment les fraises \n",
            "\n",
            "Predicted: ils aiment les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  we like pears \n",
            "\n",
            "Ground truth:  nous aimons les poires \n",
            "\n",
            "Predicted: nous aimons les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  we like bananas \n",
            "\n",
            "Ground truth:  nous aimons les bananes \n",
            "\n",
            "Predicted: nous aimons les citrons \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  my most loved fruit is the mango \n",
            "\n",
            "Ground truth:  mon fruit le plus aime est la mangue \n",
            "\n",
            "Predicted: mon fruit le plus aime est lorange \n",
            "\n",
            "--------------------\n",
            "\n",
            "Input:  you like pears \n",
            "\n",
            "Ground truth:  vous aimez les poires \n",
            "\n",
            "Predicted: vous aimez les citrons \n",
            "\n",
            "--------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEHcCdQFeCX6",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "Since our model is under-fitting, getting more data would not be too helpful, instead adding more layers to the network is a possible solution combined with regularization methods (like Dropout) to prevent over-fitting. The key takeaway from this project is that even though i've used a simple model (with a single LSTM layer), the encoder-decoder architecture could be succesfully used for a task like Neural Machine Translation."
      ]
    }
  ]
}